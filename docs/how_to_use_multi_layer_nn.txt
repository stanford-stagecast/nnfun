Creation:
auto nn = make_unique<Network<float, batch_size, input_size, num_layer1_nodes, ..., num_last_layer_nodes, output_size>>();
- Ignore 'float' and 'batch_size' for current usages.
- 'input_size' refers to the number of inputs to the neural network.
- 'output_size' refers to the number of outputs to the neurla network.
- 'num_layer1_nodes, ..., num_last_layer_nodes' each refer to the number of nodes in the interim layers.
For example, if we want the nn to be: 1->16->10->20, here is how to create:
auto nn = make_unique<Network<float, batch_size, 1, 16, 10, 20>>();

Initialization:
For the sake of the project, we want to use random initialization.
nn->layer0.initializeWeightsRandomly();  // this line randomly initializes the first layer
nn->next.layer0.initializeWeightsRandomly(); // this line randomly initializes the second layer
....
It would be convenient to use a loop if there are multiple layers.
Note that initializeWeightsRandomly() also initializes biases randomly.

Compute:
nn->apply( input );
Note that 'input' should be of type Matrix, with dimension the same as input_size.
After applying the input to the nn, `nn->output()` will return the output with dimention output_size.

Backpropagate:
nn->computeDeltas();
nn->evaluateGradients( input );
nn->getEvaluatedGradient( layernum, which_gradient );

Change weights:
nn->layer0.weights(); // this line returns the weight matrix of the first layer
nn->layer0.biases(); // this line returns the biase matrix of the first layer
nn->next.layer0.weights(); // this line returns the weight matrix of the second layer
nn->next.layer0.biases(); // this line returns the biase matrix of the second layer
....
Similar to the initialization part, it would be convenient to use for loop for multiple layers.
For example, if we want to change the first weight of the second layer to 1:
nn->next.layer0.weights()( 0 ) = 1;

Beautiful print:
nn->print();
